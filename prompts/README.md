# Руководство по модулям промптов (Prompts)

## Обзор

Модули промптов представляют собой набор интеллектуальных инструментов, которые позволяют системе взаимодействовать с языковой моделью GigaChat для решения трех ключевых задач:

- **Категоризация вопросов** - автоматическое определение тематической категории вопроса
- **Генерация ответов** - создание информативных ответов на основе контекста
- **Оценка качества** - сравнение сгенерированных ответов с эталонными

## Структура модулей

### 1. Модуль категоризации вопросов (get_category_prompt)

**Назначение**: Автоматически определяет, к какой тематической категории относится заданный вопрос.

**Где используется**:
- В скрипте `category_filler.py` для заполнения категорий в базе вопросов-ответов
- В скрипте `answer_generator.py` для определения категории нового вопроса

**Основные настройки**:

- **categories** - словарь доступных категорий
  - Ключ: название категории (например, "Technical")
  - Значение: описание категории (например, "Вопросы о технической реализации")

- **Параметры модели**:
  - Все параметры в данный момент закомментированы в коде
  - Используются значения по умолчанию GigaChat
  - При необходимости можно раскомментировать:
    - `model`: "GigaChat-2-Pro" или "GigaChat"
    - `stream`: False
    - `profanity_check`: False

**Формат результата**:
```json
{
  "category": "Technical",
  "confidence": 0.95,
  "reasoning": "Вопрос касается технической реализации системы"
}
```

### 2. Модуль генерации ответов (get_answer_prompt)

**Назначение**: Генерирует подробные ответы на вопросы, используя контекст из похожих вопросов-ответов.

**Где используется**:
- В скрипте `answer_generator.py` для создания ответов на новые вопросы

**Основные настройки**:

- **topic** - общая тема конференции или мероприятия (опционально)
  - Пример: "Машинное обучение в производстве"
  - Помогает сфокусировать ответы на нужной тематике

- **knowledge_base** - файл базы знаний
  - Имя файла: `get_answer_prompt_kbase.txt`
  - Ищется в директории модуля
  - Содержит дополнительную справочную информацию
  - Загружается автоматически при первом использовании

- **Режимы работы с контекстом**:
  - `use_chat_history`: True - контекст передается как история диалога
  - `use_chat_history`: False - контекст передается в одном сообщении пользователя

- **Параметры модели**:
  - Все параметры в данный момент закомментированы в коде
  - Используются значения по умолчанию GigaChat
  - При необходимости можно раскомментировать:
    - `model`: "GigaChat-2-Pro" или "GigaChat"
    - `stream`: False
    - `profanity_check`: False

**Входные данные (qa_pairs)**:
- Список похожих вопросов-ответов из базы
- Каждая пара содержит:
  - `question`: текст вопроса (обязательный)
  - `answer`: текст ответа (обязательный)
  - `similarity`: степень похожести 0.0-1.0 (опциональный, NotRequired)

**Формат результата**:
```json
{
  "answer": "Подробный ответ на вопрос",
  "confidence": 0.85,
  "sources_used": ["context"],
  "sources_used_reasoning": "Использован контекст из диалога. Информация взята из предоставленных Q&A пар."
}
```

**Валидация sources_used**:
- Массив из 1-2 уникальных источников
- Возможные значения: "context", "domain_knowledge"
- Обязательное поле, минимум 1 элемент

### 3. Модуль оценки ответов (get_judgement_prompt)

**Назначение**: Оценивает качество сгенерированного ответа путем сравнения с эталонным. Использует технику few-shot learning с 14 калибровочными примерами для обеспечения консистентной оценки.

**Где используется**:
- В скрипте `judge_answer.py` для оценки качества сгенерированных ответов

**Основные настройки**:

- **Параметры оценки**:
  - `SCORE_SCALE`: 100 - максимальный балл
  - `THRESHOLD_GOOD`: 85 - порог для оценки "хорошо"
  - `THRESHOLD_OK`: 70 - порог для оценки "удовлетворительно"
  - `JUSTIFICATION_MAX_WORDS`: 40 - максимум слов в обосновании
  - `EVIDENCE_MAX_ITEMS`: 2 - максимум цитат для доказательств

- **Допуски для чисел**:
  - `NUMERICAL_TOLERANCE_RELATIVE`: 0.02 (2%) - относительная погрешность
  - `NUMERICAL_TOLERANCE_ABSOLUTE`: 0.000001 - абсолютная погрешность

- **Штрафы**:
  - `CONTRADICTION_PENALTY`: 0.20 - штраф за противоречия
  - `HALLUCINATION_PENALTY`: 0.10 - штраф за выдумки

- **Параметры модели** (активные):
  - `model`: "GigaChat-2-Pro"
  - `temperature`: 0.0 - максимальная детерминированность
  - `top_p`: 1.0
  - `stream`: False
  - `profanity_check`: False
  - `repetition_penalty`: 1.0

**Входные данные**:
- `question`: исходный вопрос
- `reference_answer`: эталонный ответ
- `candidate_answer`: оцениваемый ответ

**Расчет итогового балла**:
- F1 = 2 × precision × recall / (precision + recall), если precision + recall > 0
- F1 = 0, если precision = recall = 0
- score = max(0, F1 × 100 - penalties)

**Формат результата**:
```json
{
  "score": 87,
  "class": "good",
  "f1": 0.9123,
  "precision_c_to_r": 0.95,
  "recall_r_to_c": 0.88,
  "contradiction": false,
  "hallucination": false,
  "justification": "Ответ покрывает основные пункты эталона",
  "evidence": [
    {"source": "reference", "quote": "цитата из эталона"},
    {"source": "candidate", "quote": "цитата из ответа"}
  ],
  "penalties": 0.0
}
```

## Запуск и тестирование

### Запуск тестов модуля категоризации

```bash
python prompts/get_category_prompt_test.py
```

**Количество тестов**: 8 основных тестовых блоков

Тест проверяет:
- Инициализацию с различными наборами категорий
- Генерацию сообщений для языковой модели
- Обработку ответов с контекстом и без
- Управление историей диалога
- Валидацию категорий и обработку регистра
- Обработку ошибок и недопустимых данных
- Полный цикл работы с API
- Парсинг JSON и резервное извлечение полей

Результаты сохраняются в папку `prompts/get_category_prompt_test/`

### Запуск тестов модуля генерации ответов

```bash
python prompts/get_answer_prompt_test.py
```

**Количество тестов**: 10 основных тестовых блоков

Тест проверяет:
- Базовую инициализацию и инициализацию с темой
- Интеграцию базы знаний
- Переключение режимов контекста (история/явный контекст)
- Генерацию полных сообщений
- Обработку пустых данных и опциональных полей
- Совместимость с TypedDict
- Персистентность режима контекста
- Парсинг JSON с новым полем sources_used_reasoning
- Полный цикл работы с реальным API

Результаты сохраняются в папку `prompts/get_answer_prompt_test/`

### Запуск тестов модуля оценки

```bash
python prompts/get_judgement_prompt_test.py
```

**Количество тестов**: 27 детальных тестов

Тест проверяет:
- Оценку идентичных ответов
- Оценку частичных совпадений
- Обнаружение противоречий
- Обнаружение галлюцинаций
- Обработку пустых ответов
- Числовые допуски и граничные условия
- Автоматические преобразования единиц измерения
- Обработку некорректных JSON-ответов
- Исключения API и валидацию входных данных
- Расчет метрик F1, точности и полноты
- Применение штрафов и пороговые классификации
- Калибровку шкалы оценки
- Обработку вложенных структур информации
- Независимость от порядка элементов

Результаты сохраняются в папку `prompts/get_judgement_prompt_test/`

## Структура выходных файлов тестов

После запуска тестов создаются следующие файлы:

**Файлы сообщений (msgs_*.txt)**:
- Содержат полные диалоги с языковой моделью
- Формат: JSON с реальными переносами строк для читаемости
- Используются для анализа взаимодействия с API

**Файлы результатов (result_*.json)**:
- Содержат ответы языковой модели
- Формат: структурированный JSON
- Показывают реальные результаты работы модулей

## Особенности использования

### Кэширование

Все модули используют кэширование:
- Системные промпты сохраняются после первой инициализации
- История диалога накапливается между вызовами
- Текущие настройки (категории, тема, режим контекста) сохраняются
- База знаний загружается один раз при первом использовании
- Для полного сброса кэша требуется перезапуск скрипта

### Обработка ошибок

Модули никогда не вызывают исключения в публичной функции `run()`:
- Всегда возвращают кортеж из трех элементов
- При ошибке первый элемент содержит JSON с ключом "error"
- Второй элемент - список отправленных сообщений
- Третий элемент - детали ответа или ошибки

### Восстановление JSON-ответов

Все модули используют библиотеку `json_repair` для восстановления поврежденных JSON-ответов от языковой модели. При невозможности восстановления применяется резервное извлечение полей через регулярные выражения с помощью внутренних функций `_extract_*`.

### Режимы контекста (только для генерации ответов)

**Режим истории диалога** (`use_chat_history=True`):
- Контекстные Q&A пары добавляются как предыдущие сообщения
- Создает иллюзию продолжения разговора
- Каждый ответ ассистента форматируется как JSON
- Подходит для связанных вопросов

**Режим явного контекста** (`use_chat_history=False`):
- Все Q&A пары передаются в одном сообщении пользователя
- Более компактный формат
- Подходит для независимых вопросов

## Требования к входным данным

### Для категоризации

- **Вопрос**: текст на русском языке (обязательный)
- **Ответ**: текст ответа для улучшения точности категоризации (опциональный)
- **Категории**: предварительно определенный словарь с описаниями (обязательный)

### Для генерации ответов

- **Вопрос**: текст на русском языке (обязательный)
- **Q&A пары**: список словарей с ключами:
  - `question`: текст вопроса (обязательный)
  - `answer`: текст ответа (обязательный)
  - `similarity`: коэффициент похожести 0.0-1.0 (опциональный, NotRequired в TypedDict)
- **Тема**: общий контекст конференции (опциональный)

### Для оценки

- **Вопрос**: исходный вопрос (обязательный)
- **Эталонный ответ**: правильный ответ для сравнения (обязательный)
- **Оцениваемый ответ**: ответ, который нужно проверить (обязательный)

## Интерпретация результатов

### Категоризация

- **confidence > 0.9**: высокая уверенность в категории
- **confidence 0.7-0.9**: средняя уверенность
- **confidence < 0.7**: низкая уверенность, рекомендуется ручная проверка
- **reasoning**: объяснение выбора категории на русском языке

### Генерация ответов

- **confidence**: уверенность в качестве ответа (0.0-1.0)
- **sources_used**:
  - `["context"]` - ответ основан только на предоставленном контексте
  - `["domain_knowledge"]` - использованы только общие знания модели
  - `["context", "domain_knowledge"]` - комбинация источников
- **sources_used_reasoning**: объяснение использования источников (2 предложения на русском)

### Оценка качества

- **score** (итоговый балл 0-100):
  - 85-100: отличный ответ (class: "good")
  - 70-84: хороший ответ (class: "ok")
  - 0-69: требует улучшения (class: "bad")

- **Метрики точности**:
  - `precision_c_to_r`: какая часть оцениваемого ответа корректна (0.0-1.0)
  - `recall_r_to_c`: какая часть эталонного ответа покрыта (0.0-1.0)
  - `f1`: гармоническое среднее точности и полноты (0.0-1.0)

- **Флаги проблем**:
  - `contradiction`: true - ответ противоречит эталону
  - `hallucination`: true - ответ содержит факты, отсутствующие в эталоне
  
- **Дополнительная информация**:
  - `justification`: краткое объяснение оценки (до 40 слов на русском)
  - `evidence`: до 2 цитат из ответов для обоснования оценки
  - `penalties`: суммарный штраф за противоречия и галлюцинации